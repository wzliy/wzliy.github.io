<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>- arvin's blog</title><meta name=Description content><meta property="og:title" content><meta property="og:description" content="高并发系统设计 为什么学习高并发系统设计 拿电商系统中的下单流程设计技术方案为例。在每秒只有一次调用的系统中，你只需要关注业务逻辑本身就好了：查询库存是否充足，如果充足，就可以到数据库中生成订单，成功后锁定库存，然后进入支付流程。
这个流程非常清晰，实现也简单，但如果要做一次秒杀的活动，配合一些运营的推广，你会发现下单操作的调用量可能达到每秒 10000 次！
10000 次请求同时查询库存，是否会把库存系统拖垮？如果请求全部通过，那么就要同时生成 10000 次订单，数据库能否抗住？如果抗不住，我们要如何做？这些问题都可能出现，并让之前的方案不再适用，此时你就需要设计新的方案。除此之外，同样是缓存的使用，在低并发下你只需要了解基本的使用方式，但在高并发场景下你需要关注缓存命中率，如何应对缓存穿透，如何避免雪崩，如何解决缓存一致性等问题，这就增加了设计方案的复杂度，对设计者能力的要求也会更高。所以，为了避免遇到问题时手忙脚乱，你有必要提前储备足够多的高并发知识，从而具备随时应对可能出现的高并发需求场景的能力。
高并发系统的通用设计方法 高并发代表着大流量，高并发系统设计的魅力就在于我们能够凭借自己的聪明才智设计巧妙的方案，从而抵抗巨大流量的冲击，带给用户更好的使用体验。这些方案好似能操纵流量，让流量更加平稳地被系统中的服务和组件处理。
我们应对高并发大流量时也会采用类似“分洪泄流”的方式，归纳起来共有三种方法：
 横向扩展：分而治之是一种常见的高并发系统设计方法，采用分布式部署的方式把流量分流开，让每个服务器都承担一部分并发和流量。 缓存：使用缓存来提高系统的性能，就好比用“拓宽河道”的方式抵抗高并发大流量的冲击。 异步解耦：在某些场景下，未处理完成之前我们可以让请求先返回，在数据准备好之后再通知请求方，这样可以在单位时间内处理更多的请求。  Scale-out 横向扩展 Scale-up与Scale-out
Scale-up类似于追逐摩尔定律不断提升CPU性能，通过购买性能更好的硬件来提升系统处理并发的能力；Scale-out类似于CPU多核心方案，通过将多个低性能的机器组成一个分布式集群来共同抵御高并发流量的冲击。
一般来讲，在我们系统设计初期会考虑使用 Scale-up 的方式，因为这种方案足够简单，所谓能用堆砌硬件解决的问题就用硬件来解决，但是当系统并发超过了单机的极限时，我们就要使用 Scale-out 的方式。
Scale-out 虽然能够突破单机的限制，但也会引入一些复杂问题。比如，如果某个节点出现故障如何保证整体可用性？当多个节点有状态需要同步时如何保证状态信息在不同节点的一致性？如何做到使用方无感知的增加和删除节点？其中每一个问题都涉及很多的知识点，我会在后面的课程中深入地讲解，这里暂时不展开了。
使用缓存提升性能
缓存遍布在系统设计的每个角落，从操作系统到浏览器，从数据库到消息队列，任何略微复杂的服务和组件中你都可以看到缓存的影子。我们使用缓存的主要作用是提升系统的访问性能，在高并发的场景下就可以支撑更多用户的同时访问。
缓存提升性能是因为数据存储在内存中，而数据库持久化存储的是存放在硬盘中的，而磁盘是整个计算机体系中最慢的一环，访问内存比访问硬盘能获的更好的性能提升。
缓存的语义已经丰富了很多，我们可以将任何降低响应时间的中间存储都称为缓存。缓存的思想遍布很多设计领域，比如在操作系统中 CPU 有多级缓存，文件有 Page Cache 缓存。
异步处理
异步也是一种常见的高并发设计方法，我们在很多文章和演讲中都能听到这个名词，与之共同出现的还有它的反义词：同步。比如分布式服务框架 Dubbo 中有同步方法调用和异步方法调用，IO 模型中有同步 IO 和异步 IO。
同步与异步
同步调用代表调用方要阻塞等待被调用方法中的逻辑执行完成。这种方式下，当被调用方法响应时间较长时，会造成调用方长久的阻塞，在高并发下会造成整体系统性能下降甚至发生雪崩。
异步调用恰恰相反，调用方不需要等待方法逻辑执行完成就可以返回执行其他的逻辑，在被调用方法执行完毕后再通过回调、事件通知等方式将结果反馈给调用方。
处理逻辑后移到异步处理程序中，Web 服务的压力小了，资源占用的少了，自然就能接收更多的用户订票请求，系统承受高并发的能力也就提升了。
系统设计的一般思路
一般系统的演进过程应该遵循下面的思路：
  最简单的系统设计满足业务需求和流量现状，选择最熟悉的技术体系。
  随着流量的增加和业务的变化修正架构中存在问题的点，如单点问题、横向扩展问题、性能无法满足需求的组件。在这个过程中，选择社区成熟的、团队熟悉的组件帮助我们解决问题，在社区没有合适解决方案的前提下才会自己造轮子。
  当对架构的小修小补无法满足需求时，考虑重构、重写等大的调整方式以解决现有的问题。
  高并发系统的演进应该是循序渐进，以解决系统中存在的问题为目的和驱动力的。
经验讨论
martin fowler好像曾经说过，能使用单体解决的问题，就不要采用分布式。不能为了技术而技术，采用分布式固然可以分流用户请求，提高系统的响应能力，但同样也带来了复杂性。软件开发最终的目的是商业利益。非常赞成老师的观点，罗马城不是一天就建立起来的。架构的工作应该是阶段性，解决阶段性系统的复杂性。如果单体跑的很好，或者通过scale up方式在成本可控的情况能解决就不要想着诗和远方，因为系统内部的进程间调用，肯定比不同物理机的进程之间调用要快。
之前做的创业项目也是遇到盲目优化的问题，系统最核心的撮合结算服务，刚开始只能100次每秒，后来为了优化到百万级，花了大量时间研究各种方案，做了大量的性能测试，耽误了很长时间推向市场，结果最终优化到了不到一万tps，但后面真正上线的结果可能不到也就100tps，所以真正的需求是市场需求，不是一开始就冲着最牛逼的方案搞，线上的需求远比一开始的预想复杂，没足够的资源和动力，绝对不要折腾，不过时刻准备好可能会出现的瓶颈是必要的，免得半夜宕机，慌得一比。"><meta property="og:type" content="article"><meta property="og:url" content="https://wzliy.github.io/posts/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"><meta property="article:section" content="posts"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="高并发系统设计 为什么学习高并发系统设计 拿电商系统中的下单流程设计技术方案为例。在每秒只有一次调用的系统中，你只需要关注业务逻辑本身就好了：查询库存是否充足，如果充足，就可以到数据库中生成订单，成功后锁定库存，然后进入支付流程。
这个流程非常清晰，实现也简单，但如果要做一次秒杀的活动，配合一些运营的推广，你会发现下单操作的调用量可能达到每秒 10000 次！
10000 次请求同时查询库存，是否会把库存系统拖垮？如果请求全部通过，那么就要同时生成 10000 次订单，数据库能否抗住？如果抗不住，我们要如何做？这些问题都可能出现，并让之前的方案不再适用，此时你就需要设计新的方案。除此之外，同样是缓存的使用，在低并发下你只需要了解基本的使用方式，但在高并发场景下你需要关注缓存命中率，如何应对缓存穿透，如何避免雪崩，如何解决缓存一致性等问题，这就增加了设计方案的复杂度，对设计者能力的要求也会更高。所以，为了避免遇到问题时手忙脚乱，你有必要提前储备足够多的高并发知识，从而具备随时应对可能出现的高并发需求场景的能力。
高并发系统的通用设计方法 高并发代表着大流量，高并发系统设计的魅力就在于我们能够凭借自己的聪明才智设计巧妙的方案，从而抵抗巨大流量的冲击，带给用户更好的使用体验。这些方案好似能操纵流量，让流量更加平稳地被系统中的服务和组件处理。
我们应对高并发大流量时也会采用类似“分洪泄流”的方式，归纳起来共有三种方法：
 横向扩展：分而治之是一种常见的高并发系统设计方法，采用分布式部署的方式把流量分流开，让每个服务器都承担一部分并发和流量。 缓存：使用缓存来提高系统的性能，就好比用“拓宽河道”的方式抵抗高并发大流量的冲击。 异步解耦：在某些场景下，未处理完成之前我们可以让请求先返回，在数据准备好之后再通知请求方，这样可以在单位时间内处理更多的请求。  Scale-out 横向扩展 Scale-up与Scale-out
Scale-up类似于追逐摩尔定律不断提升CPU性能，通过购买性能更好的硬件来提升系统处理并发的能力；Scale-out类似于CPU多核心方案，通过将多个低性能的机器组成一个分布式集群来共同抵御高并发流量的冲击。
一般来讲，在我们系统设计初期会考虑使用 Scale-up 的方式，因为这种方案足够简单，所谓能用堆砌硬件解决的问题就用硬件来解决，但是当系统并发超过了单机的极限时，我们就要使用 Scale-out 的方式。
Scale-out 虽然能够突破单机的限制，但也会引入一些复杂问题。比如，如果某个节点出现故障如何保证整体可用性？当多个节点有状态需要同步时如何保证状态信息在不同节点的一致性？如何做到使用方无感知的增加和删除节点？其中每一个问题都涉及很多的知识点，我会在后面的课程中深入地讲解，这里暂时不展开了。
使用缓存提升性能
缓存遍布在系统设计的每个角落，从操作系统到浏览器，从数据库到消息队列，任何略微复杂的服务和组件中你都可以看到缓存的影子。我们使用缓存的主要作用是提升系统的访问性能，在高并发的场景下就可以支撑更多用户的同时访问。
缓存提升性能是因为数据存储在内存中，而数据库持久化存储的是存放在硬盘中的，而磁盘是整个计算机体系中最慢的一环，访问内存比访问硬盘能获的更好的性能提升。
缓存的语义已经丰富了很多，我们可以将任何降低响应时间的中间存储都称为缓存。缓存的思想遍布很多设计领域，比如在操作系统中 CPU 有多级缓存，文件有 Page Cache 缓存。
异步处理
异步也是一种常见的高并发设计方法，我们在很多文章和演讲中都能听到这个名词，与之共同出现的还有它的反义词：同步。比如分布式服务框架 Dubbo 中有同步方法调用和异步方法调用，IO 模型中有同步 IO 和异步 IO。
同步与异步
同步调用代表调用方要阻塞等待被调用方法中的逻辑执行完成。这种方式下，当被调用方法响应时间较长时，会造成调用方长久的阻塞，在高并发下会造成整体系统性能下降甚至发生雪崩。
异步调用恰恰相反，调用方不需要等待方法逻辑执行完成就可以返回执行其他的逻辑，在被调用方法执行完毕后再通过回调、事件通知等方式将结果反馈给调用方。
处理逻辑后移到异步处理程序中，Web 服务的压力小了，资源占用的少了，自然就能接收更多的用户订票请求，系统承受高并发的能力也就提升了。
系统设计的一般思路
一般系统的演进过程应该遵循下面的思路：
  最简单的系统设计满足业务需求和流量现状，选择最熟悉的技术体系。
  随着流量的增加和业务的变化修正架构中存在问题的点，如单点问题、横向扩展问题、性能无法满足需求的组件。在这个过程中，选择社区成熟的、团队熟悉的组件帮助我们解决问题，在社区没有合适解决方案的前提下才会自己造轮子。
  当对架构的小修小补无法满足需求时，考虑重构、重写等大的调整方式以解决现有的问题。
  高并发系统的演进应该是循序渐进，以解决系统中存在的问题为目的和驱动力的。
经验讨论
martin fowler好像曾经说过，能使用单体解决的问题，就不要采用分布式。不能为了技术而技术，采用分布式固然可以分流用户请求，提高系统的响应能力，但同样也带来了复杂性。软件开发最终的目的是商业利益。非常赞成老师的观点，罗马城不是一天就建立起来的。架构的工作应该是阶段性，解决阶段性系统的复杂性。如果单体跑的很好，或者通过scale up方式在成本可控的情况能解决就不要想着诗和远方，因为系统内部的进程间调用，肯定比不同物理机的进程之间调用要快。
之前做的创业项目也是遇到盲目优化的问题，系统最核心的撮合结算服务，刚开始只能100次每秒，后来为了优化到百万级，花了大量时间研究各种方案，做了大量的性能测试，耽误了很长时间推向市场，结果最终优化到了不到一万tps，但后面真正上线的结果可能不到也就100tps，所以真正的需求是市场需求，不是一开始就冲着最牛逼的方案搞，线上的需求远比一开始的预想复杂，没足够的资源和动力，绝对不要折腾，不过时刻准备好可能会出现的瓶颈是必要的，免得半夜宕机，慌得一比。"><meta name=application-name content="arvin's blog"><meta name=apple-mobile-web-app-title content="arvin's blog"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://wzliy.github.io/posts/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/><link rel=next href=https://wzliy.github.io/posts/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/wzliy.github.io\/posts\/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1\/"},"genre":"posts","wordcount":537,"url":"https:\/\/wzliy.github.io\/posts\/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1\/","publisher":{"@type":"Organization","name":"作者"},"author":{"@type":"Person","name":"作者"},"description":""}</script></head><body header-desktop header-mobile><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':''==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:''==='dark')&&document.body.setAttribute('theme','dark')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="arvin's blog">arvin's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="arvin's blog">arvin's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX"></h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=" author" class=author><i class="fas fa-user-circle fa-fw"></i>作者</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=0001-01-01>0001-01-01</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 537 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 3 分钟&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#为什么学习高并发系统设计>为什么学习高并发系统设计</a></li><li><a href=#高并发系统的通用设计方法>高并发系统的通用设计方法</a></li><li><a href=#架构分层>架构分层</a></li><li><a href=#系统设计目标>系统设计目标</a><ul><li><a href=#如何提升系统性能>如何提升系统性能</a></li><li><a href=#系统怎样做到高可用>系统怎样做到高可用</a></li><li><a href=#如何让系统易于扩展>如何让系统易于扩展</a></li></ul></li><li><a href=#数据库篇>数据库篇</a><ul><li><a href=#数据库连接池>数据库连接池</a></li><li><a href=#数据库优化方案一主从分离>数据库优化方案一：主从分离</a></li></ul></li></ul></nav></div></div><div class=content id=content><h1 id=高并发系统设计>高并发系统设计</h1><h2 id=为什么学习高并发系统设计>为什么学习高并发系统设计</h2><p>拿电商系统中的下单流程设计技术方案为例。在每秒只有一次调用的系统中，你只需要关注业务逻辑本身就好了：查询库存是否充足，如果充足，就可以到数据库中生成订单，成功后锁定库存，然后进入支付流程。</p><p>这个流程非常清晰，实现也简单，但如果要做一次秒杀的活动，配合一些运营的推广，你会发现下单操作的调用量可能达到每秒 10000 次！</p><p>10000 次请求同时查询库存，是否会把库存系统拖垮？如果请求全部通过，那么就要同时生成 10000 次订单，数据库能否抗住？如果抗不住，我们要如何做？这些问题都可能出现，并让之前的方案不再适用，此时你就需要设计新的方案。除此之外，同样是缓存的使用，在低并发下你只需要了解基本的使用方式，但在高并发场景下你需要关注缓存命中率，如何应对缓存穿透，如何避免雪崩，如何解决缓存一致性等问题，这就增加了设计方案的复杂度，对设计者能力的要求也会更高。所以，为了避免遇到问题时手忙脚乱，你有必要提前储备足够多的高并发知识，从而具备随时应对可能出现的高并发需求场景的能力。</p><h2 id=高并发系统的通用设计方法>高并发系统的通用设计方法</h2><p>高并发代表着大流量，高并发系统设计的魅力就在于我们能够凭借自己的聪明才智设计巧妙的方案，从而抵抗巨大流量的冲击，带给用户更好的使用体验。这些方案好似能操纵流量，让流量更加平稳地被系统中的服务和组件处理。</p><p>我们应对高并发大流量时也会采用类似“分洪泄流”的方式，归纳起来共有三种方法：</p><ul><li>横向扩展：分而治之是一种常见的高并发系统设计方法，采用分布式部署的方式把流量分流开，让每个服务器都承担一部分并发和流量。</li><li>缓存：使用缓存来提高系统的性能，就好比用“拓宽河道”的方式抵抗高并发大流量的冲击。</li><li>异步解耦：在某些场景下，未处理完成之前我们可以让请求先返回，在数据准备好之后再通知请求方，这样可以在单位时间内处理更多的请求。</li></ul><p><strong>Scale-out 横向扩展</strong>
Scale-up与Scale-out</p><p>Scale-up类似于追逐摩尔定律不断提升CPU性能，通过购买性能更好的硬件来提升系统处理并发的能力；Scale-out类似于CPU多核心方案，通过将多个低性能的机器组成一个分布式集群来共同抵御高并发流量的冲击。</p><p>一般来讲，在我们系统设计初期会考虑使用 Scale-up 的方式，因为这种方案足够简单，所谓能用堆砌硬件解决的问题就用硬件来解决，但是当系统并发超过了单机的极限时，我们就要使用 Scale-out 的方式。</p><p>Scale-out 虽然能够突破单机的限制，但也会引入一些复杂问题。比如，如果某个节点出现故障如何保证整体可用性？当多个节点有状态需要同步时如何保证状态信息在不同节点的一致性？如何做到使用方无感知的增加和删除节点？其中每一个问题都涉及很多的知识点，我会在后面的课程中深入地讲解，这里暂时不展开了。</p><p><strong>使用缓存提升性能</strong></p><p>缓存遍布在系统设计的每个角落，从操作系统到浏览器，从数据库到消息队列，任何略微复杂的服务和组件中你都可以看到缓存的影子。我们使用缓存的主要作用是提升系统的访问性能，在高并发的场景下就可以支撑更多用户的同时访问。</p><p>缓存提升性能是因为数据存储在内存中，而数据库持久化存储的是存放在硬盘中的，而磁盘是整个计算机体系中最慢的一环，访问内存比访问硬盘能获的更好的性能提升。</p><p>缓存的语义已经丰富了很多，我们可以将任何降低响应时间的中间存储都称为缓存。缓存的思想遍布很多设计领域，比如在操作系统中 CPU 有多级缓存，文件有 Page Cache 缓存。</p><p><strong>异步处理</strong></p><p>异步也是一种常见的高并发设计方法，我们在很多文章和演讲中都能听到这个名词，与之共同出现的还有它的反义词：同步。比如分布式服务框架 Dubbo 中有同步方法调用和异步方法调用，IO 模型中有同步 IO 和异步 IO。</p><p>同步与异步</p><p>同步调用代表调用方要阻塞等待被调用方法中的逻辑执行完成。这种方式下，当被调用方法响应时间较长时，会造成调用方长久的阻塞，在高并发下会造成整体系统性能下降甚至发生雪崩。</p><p>异步调用恰恰相反，调用方不需要等待方法逻辑执行完成就可以返回执行其他的逻辑，在被调用方法执行完毕后再通过回调、事件通知等方式将结果反馈给调用方。</p><p>处理逻辑后移到异步处理程序中，Web 服务的压力小了，资源占用的少了，自然就能接收更多的用户订票请求，系统承受高并发的能力也就提升了。</p><p><strong>系统设计的一般思路</strong></p><p>一般系统的演进过程应该遵循下面的思路：</p><ul><li><p>最简单的系统设计满足业务需求和流量现状，选择最熟悉的技术体系。</p></li><li><p>随着流量的增加和业务的变化修正架构中存在问题的点，如单点问题、横向扩展问题、性能无法满足需求的组件。在这个过程中，选择社区成熟的、团队熟悉的组件帮助我们解决问题，在社区没有合适解决方案的前提下才会自己造轮子。</p></li><li><p>当对架构的小修小补无法满足需求时，考虑重构、重写等大的调整方式以解决现有的问题。</p></li></ul><p>高并发系统的演进应该是循序渐进，以解决系统中存在的问题为目的和驱动力的。</p><p><strong>经验讨论</strong></p><p>martin fowler好像曾经说过，能使用单体解决的问题，就不要采用分布式。不能为了技术而技术，采用分布式固然可以分流用户请求，提高系统的响应能力，但同样也带来了复杂性。软件开发最终的目的是商业利益。非常赞成老师的观点，罗马城不是一天就建立起来的。架构的工作应该是阶段性，解决阶段性系统的复杂性。如果单体跑的很好，或者通过scale up方式在成本可控的情况能解决就不要想着诗和远方，因为系统内部的进程间调用，肯定比不同物理机的进程之间调用要快。</p><p>之前做的创业项目也是遇到盲目优化的问题，系统最核心的撮合结算服务，刚开始只能100次每秒，后来为了优化到百万级，花了大量时间研究各种方案，做了大量的性能测试，耽误了很长时间推向市场，结果最终优化到了不到一万tps，但后面真正上线的结果可能不到也就100tps，所以真正的需求是市场需求，不是一开始就冲着最牛逼的方案搞，线上的需求远比一开始的预想复杂，没足够的资源和动力，绝对不要折腾，不过时刻准备好可能会出现的瓶颈是必要的，免得半夜宕机，慌得一比。</p><p>高并发除了横向扩容，缓存和异步化，系统还需要做好保护，比如限流降级，过载保护。甚至高并发的问题更是一个系统性问题，从前端到服务端，从产品设计上都要考虑进来。不过这块就比较业务化了，不是常规的操作</p><p>1.技术在不断演进，演进的目的和内驱动力是解决当前系统存在的问题，过早过度设计大多只会延误系统的发展。一切都以实际情况和需要出发，一步步优化，一步步演进，个人能力提升也是同样的道理。
2.高并发系统设计通用方法:水平拓展，缓存，异步。这只是指导思想，如何更巧妙的运用才是最具魅力的。</p><p><strong>个人总结</strong>
高并发系统设计的主要目的就是为了应对大流量的冲击，也就是提升响应，简单的可以通过提升服务器的性能来实现，但单台服务器的性能不可能永远提升，所以，就引入了分布系统，通过多台服务器的集群设计来分摊整个流量，这是从整体系统的提升来看，从每个请求来看，可以通过缓存，异步处理的方式提升单个请求的响应，从而提升整个系统的响应。</p><p>所以，宏观方面就是使用分布式系统，微观方面就是缓存和异步解耦。但引入了分布式系统，缓存和异步解耦又会提升整个系统的复杂性，所以不能盲目追求系统的高并发，高并发系统的演进应该式循序渐进的，但在系统设计之初可以多考虑以后的设计，为以后系统演进留下空间。</p><h2 id=架构分层>架构分层</h2><p>在系统从 0 到 1 的阶段，为了让系统快速上线，我们通常是不考虑分层的。但是随着业务越来越复杂，大量的代码纠缠在一起，会出现逻辑不清晰、各模块相互依赖、代码扩展性差、改动一处就牵一发而动全身等问题。</p><p>架构分层和高并发系统的关系</p><p><strong>分层的好处</strong></p><p>分层的设计可以简化系统设计，让不同的人专注做某一层次的事情。</p><p>分层之后可以复用。</p><p>分层架构可以让我们更方便的横向扩展。</p><p><strong>如何做系统分层</strong></p><p>最主要的一点就是你需要理清楚每个层次的边界是什么。</p><p>分层架构需要考虑层次之间一定是相邻层互相依赖，数据的流转也只能在相邻的两层之间流转。</p><p>分层架构是软件设计思想的外在体现，是一种实现方式。我们熟知的一些软件设计原则都在分层架构中有所体现。</p><p>比方说，</p><ul><li><p>单一职责原则规定每个类只有单一的功能，在这里可以引申为每一层拥有单一职责，且层与层之间边界清晰；</p></li><li><p>迪米特法则原意是一个对象应当对其它对象有尽可能少的了解，在分层架构的体现是数据的交互不能跨层，只能在相邻层之间进行；</p></li><li><p>而开闭原则要求软件对扩展开放，对修改关闭。它的含义其实就是将抽象层和实现层分离，抽象层是对实现层共有特征的归纳总结，不可以修改，但是具体的实现是可以无限扩展，随意替换的。</p></li></ul><p><strong>经验讨论</strong></p><p>明白了一些东西，老师比如我做一个接口，这个实现可以放在server层！ 之后公司内部调用逻辑可以放在web层！而哪一天公司要开放这个接口，那我最好是新抽象一层出来(一个新的服务)就是开放平台层！这样做的好处是，可以讲自家使用和第三方使用做隔离！比如在提供服务时，为了保证自家接口性能，对开放平台层做限流处理！</p><p>有人说传统的 MVC 是贫血模式，与之对立的是充血模式，边界划分衍生出 DDD ，优劣到底是哪一个，弄不清楚呐，老师的观点是什么？</p><p>贫血模型把数据和行为区分开，层次上更清晰，但是呢不够面向对象，因为对象肯定是有数据和行为的
充血模型是把数据和行为混在一起，缺点是我就不知道业务逻辑是写在biz层还是领域模型里面。
我更倾向于贫血模型吧，因为我觉得大团队里面很难规范哪些放在领域模型里面，哪些放在biz层，所以干脆就都放在biz层就好了</p><p>mvc这种结构让太多的人觉得项目工程结构理所应当就是这样的，然后呢，一大堆的业务逻辑就随意的堆砌在了service中，对象啥的，只是单纯的数据传输作用，出现了用面向对象的语言，写面向过程的程序的普遍现象。按照领域驱动设计的思路，最重要的还要有领域模型层。当然manage层这种方案也是一种思路，但是我觉得，这种方式，还不够，必须有清晰的业务模型和合理的分层结构配合，才能更好的提现分层的作用。</p><h2 id=系统设计目标>系统设计目标</h2><p><strong>高并发系统设计的三大目标：高性能、高可用、可扩展</strong></p><p>高并发，是指运用设计手段让系统能够处理更多的用户并发请求，也就是承担更大的流量。它是一切架构设计的背景和前提，脱离了它去谈性能和可用性是没有意义的。</p><p>而性能和可用性，是我们实现高并发系统设计必须考虑的因素。</p><p>可扩展性同样是高并发系统设计需要考虑的因素。易于扩展的系统能在短时间内迅速完成扩容，更加平稳地承担峰值流量。</p><h3 id=如何提升系统性能>如何提升系统性能</h3><h4 id=性能的优化原则>性能的优化原则</h4><p>性能优化原则：</p><p><strong>性能优化是问题导向的</strong></p><p>脱离了问题，盲目地提早优化会增加系统的复杂度，浪费开发人员的时间，也因为某些优化可能会对业务上有些折中的考虑，所以也会损伤业务。</p><p><strong>性能优化遵循八二原则</strong></p><p>你可以用 20% 的精力解决 80% 的性能问题。所以我们在优化过程中一定要抓住主要矛盾，优先优化主要的性能瓶颈点。</p><p><strong>性能优化需要数据支撑</strong></p><p>在优化过程中，你要时刻了解你的优化让响应时间减少了多少，提升了多少的吞吐量。</p><p><strong>性能优化的过程是持续的</strong></p><p>高并发的系统通常是业务逻辑相对复杂的系统，那么在这类系统中出现的性能问题通常也会有多方面的原因。因此，我们在做性能优化的时候要明确目标，比方说，支撑每秒 1 万次请求的吞吐量下响应时间在 10ms，那么我们就需要持续不断地寻找性能瓶颈，制定优化方案，直到达到目标为止。</p><p>在以上四个原则的指引下，掌握常见性能问题的排查方式和优化手段，就一定能让你在设计高并发系统时更加游刃有余。</p><h4 id=性能的度量指标>性能的度量指标</h4><p>对于性能我们需要有度量的标准，有了数据才能明确目前存在的性能问题，也能够用数据来评估性能优化的效果。所以明确性能的度量指标十分重要。</p><p>一般来说，度量性能的指标是系统接口的响应时间，但是单次的响应时间是没有意义的，你需要知道一段时间的性能情况是什么样的。所以，我们需要收集这段时间的响应时间数据，然后依据一些统计方法计算出特征值，这些特征值就能够代表这段时间的性能情况。我们常见的特征值有以下几类。</p><ul><li><p>平均值</p><p>平均值是把这段时间所有请求的响应时间数据相加，再除以总请求数。平均值可以在一定程度上反应这段时间的性能，但它敏感度比较差，如果这段时间有少量慢请求时，在平均值上并不能如实地反应。平均值对于度量性能来说只能作为一个参考。</p></li><li><p>最大值</p><p>就是这段时间内所有请求响应时间最长的值，但它的问题又在于过于敏感了。</p></li><li><p>分位值</p><p>分位值有很多种，比如 90 分位、95 分位、75 分位。以 90 分位为例，我们把这段时间请求的响应时间从小到大排序，假如一共有 100 个请求，那么排在第 90 位的响应时间就是 90 分位值。分位值排除了偶发极慢请求对于数据的影响，能够很好地反应这段时间的性能情况，分位值越大，对于慢请求的影响就越敏感。</p></li></ul><p>分位值是最适合作为时间段内，响应时间统计值来使用的，在实际工作中也应用最多。除此之外，平均值也可以作为一个参考值来使用。</p><p><strong>吞吐量与响应时间</strong></p><p>我们通常使用吞吐量或者响应时间来度量并发和流量，使用吞吐量的情况会更多一些。但是你要知道，这两个指标是呈倒数关系的。</p><p>一般我们度量性能时都会同时兼顾吞吐量和响应时间，比如我们设立性能优化的目标时通常会这样表述：在每秒 1 万次的请求量下，响应时间 99 分位值在 10ms 以下。</p><p>响应时间究竟控制在多长时间比较合适呢？</p><p>从用户使用体验的角度来看，200ms 是第一个分界点：接口的响应时间在 200ms 之内，用户是感觉不到延迟的，就像是瞬时发生的一样。而 1s 是另外一个分界点：接口的响应时间在 1s 之内时，虽然用户可以感受到一些延迟，但却是可以接受的，超过 1s 之后用户就会有明显等待的感觉，等待时间越长，用户的使用体验就越差。所以，健康系统的 99 分位值的响应时间通常需要控制在 200ms 之内，而不超过 1s 的请求占比要在 99.99% 以上。</p><h4 id=高并发下的性能优化>高并发下的性能优化</h4><p>随着并发的增长我们实现高性能的思路是怎样的。</p><h5 id=提高系统的处理核心数>提高系统的处理核心数</h5><p>提高系统的处理核心数就是增加系统的并行处理能力，这个思路是优化性能最简单的途径。</p><p>是不是无限制地增加处理核心数就能无限制地提升性能，从而提升系统处理高并发的能力呢？很遗憾，随着并发进程数的增加，并行的任务对于系统资源的争抢也会愈发严重。在某一个临界点上继续增加并发进程数，反而会造成系统性能的下降，这就是性能测试中的拐点模型。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://static001.geekbang.org/resource/image/23/3f/2379fce36fa3453a0326e62e4d5a333f.jpg data-srcset="https://static001.geekbang.org/resource/image/23/3f/2379fce36fa3453a0326e62e4d5a333f.jpg, https://static001.geekbang.org/resource/image/23/3f/2379fce36fa3453a0326e62e4d5a333f.jpg 1.5x, https://static001.geekbang.org/resource/image/23/3f/2379fce36fa3453a0326e62e4d5a333f.jpg 2x" data-sizes=auto alt=https://static001.geekbang.org/resource/image/23/3f/2379fce36fa3453a0326e62e4d5a333f.jpg title=img></p><p>并发用户数处于轻压力区时，响应时间平稳，吞吐量和并发用户数线性相关。而当并发用户数处于重压力区时，系统资源利用率到达极限，吞吐量开始有下降的趋势，响应时间也会略有上升。这个时候，再对系统增加压力，系统就进入拐点区，处于超负荷状态，吞吐量下降，响应时间大幅度上升。</p><p>所以我们在评估系统性能时通常需要做压力测试，目的就是找到系统的“拐点”，从而知道系统的承载能力，也便于找到系统的瓶颈，持续优化系统性能。</p><h5 id=减少单次任务响应时间>减少单次任务响应时间</h5><p>想要减少任务的响应时间，首先要看你的系统是 CPU 密集型还是 IO 密集型的，因为不同类型的系统性能优化方式不尽相同。</p><p>如何发现以及如何优化</p><p>CPU密集型：需要处理大量的 CPU 运算，那么选用更高效的算法或者减少运算次数就是这类系统重要的优化手段。</p><p>IO密集型：系统的大部分操作是在等待 IO 完成，我们熟知的系统大部分都属于 IO 密集型，比如数据库系统、缓存系统、Web 系统。这类系统的性能瓶颈可能出在系统内部，也可能是依赖的其他系统，而发现这类性能瓶颈的手段主要有两类。</p><p>第一类是采用工具：Linux 的工具集很丰富，完全可以满足你的优化需要，比如网络协议栈、网卡、磁盘、文件系统、内存，等等。这些工具的用法很多，你可以在排查问题的过程中逐渐积累。除此之外呢，一些开发语言还有针对语言特性的分析工具，比如说 Java 语言就有其专属的内存分析工具。</p><p>第二类通过监控：在监控中我们可以对任务的每一个步骤做分时的统计，从而找到任务的哪一步消耗了更多的时间。这一部分在演进篇中会有专门的介绍，这里就不再展开了。</p><p><strong>如何优化</strong></p><p>优化方案会随着问题的不同而不同。比方说，如果是数据库访问慢，那么就要看是不是有锁表的情况、是不是有全表扫描、索引加的是否合适、是否有 JOIN 操作、需不需要加缓存，等等；如果是网络的问题，就要看网络的参数是否有优化的空间，抓包来看是否有大量的超时重传，网卡是否有大量丢包等。</p><p>总而言之，“兵来将挡水来土掩”，我们需要制定不同的性能优化方案来应对不同的性能问题。</p><p>有时候你在遇到性能问题的时候会束手无策，从今天的课程中你可以得到一些启示，在这里我给你总结出几点：</p><ul><li><p>数据优先，你做一个新的系统在上线之前一定要把性能监控系统做好；</p></li><li><p>掌握一些性能优化工具和方法，这就需要在工作中不断地积累；</p></li><li><p>计算机基础知识很重要，比如说网络知识、操作系统知识等等，掌握了基础知识才能让你在优化过程中抓住性能问题的关键，也能在性能优化过程中游刃有余。</p></li></ul><p><strong>讨论思考</strong></p><p>1.业务价值->承载高并发->性能优化。一切的前提是业务价值需要。如果没有足够的价值，那么可读性才是第一，性能在需要的地方是no.1，但不需要的地方可能就是倒数第一稞。当下技术框架出来的软件差不到哪去，没有这种及时响应诉求的地方，削峰下慢慢跑就是了。（工作需要，常在缺少价值的地方着手性能优化，让我对这种就为个数字的操作很反感。要知道，异步，并发编程，逻辑缓存，算法真的会加剧系统的复杂度，得不偿失。如果没那个价值，简单才是王道）</p><p>2.提高并发度。要么加硬件，要么降低服务响应时间。做为开发，我们的目光更聚焦在降低响应时间这块。</p><p>1.采用非阻塞的rpc调用（高效的远端请求模式，采用容器的覆盖网络我认为也算）
2.将计算密集和io密集的的逻辑分割开，单独线程池，调整线程比例压榨单机性能（或者说找拐点）。
3.做缓存，io耗时的缓存和计算耗时的缓存（多级缓存，数据压缩降低带宽）。
4.采用享元模式，用好对象池和本地线程空间，尽量减少对象创建与销毁的开销，提高复用。
5.业务拆分，像状态变化后的外部系统通知，业务监控，es或solr等副本数据同步等操作，无需在主流程中做的事都拆掉。走canal监听表数据变化，推mq保最终 一致的方式从业务项目完全解偶出来。
6.fork_join，分而治之的处理大任务。并发编程，采用多线程并行的方式处理业务。（规避伪共享，减小锁力度，采用合适的锁）。
7.数据库配置优化，查询优化。（存储优化比较头疼，毕竟不按业务拆单点跑不掉，单点性能就要命。基本只能内存库先行，后台同步数据做持久。然后内存库多副本，自修复，保留一系列自修复失败的修复手段）</p><p>高并发：高性能（响应时间）、高可用（down机、故障、维护）、可扩展（应急扩容）
响应时间（平均值、最大值、分位值），响应为1s，吞吐量为每秒1次，响应缩短到10ms，吞吐量上升到每秒100次，从用户体验来说：200ms分界点，1s为另一个分界点，健康系统的99分位值的响应时间控制在200ms以内，不超过1s的请求占比要超过99.99%
高并发下的性能优化手段：
1.提高系统的处理核心数（吞吐量=核心数(并发进程数)/响应时间(s)）
但并非无限增加核心数就可以增加吞吐量，随着进程数增加，并行的任务对于资源的争夺也增加，在某
个临界点，进程增加导致系统的性能下降，这就是性能测试中的拐点模型，所以在评估系统性能时，需要做压力测试，找到拐点
2.减少单次任务响应时间
cpu密集型：优化算法
io密集型：1.采用工具，linux的工具集
2.通过监控，对任务的每一个步骤做分时统计，从而找到任务中哪一步小号消耗了更多的时间</p><p>之前做广告检索遇到的问题，倒排索引存在Redis，每次都要请求Redis，但是并发时，Redis连接数太大，甚至打开文件数过大，后采用Redis连接池，Redis连接数得到控制，而且响应更快，后来随着并发数的增大，连接池资源耗尽，而且Redis也有并发限制，数据传输导致大量占用带宽，响应时间更久，因此，又使用了本地缓存，每次请求先请求本地缓存，找不到再请求Redis，缓存到本地，缓存更新时通过消息队列来通知程序更新本地缓存，这样节省了大量的和Redis之间的请求耗时和带宽占用，性能有了数倍的提升。后面还有很多优化，性能优化不是一蹴而就的，每个阶段面对的场景是不一样的，需要找到每个瓶颈点针对性的优化。</p><p>从小到大从浅入深，老师我最想知道的是开发的系统如何找出程序瓶颈，问题具体出现在哪，用什么工具或者方法解决了，从而在后期有机会设计高并发高可用系统的时候根据实际情况来下手</p><p>找问题的话主要有三个方法，监控，工具和压测</p><p>压测和监控后面会有介绍，工具要依靠自己的积累，在文章中也有介绍</p><p>从全局看，高性能需要全链路检查，常用方法是拆分，精简，换硬件等。拆分如读写分离，分片等，精简如进程→线程→协程，HTTP→RPC，换硬件如内存替换硬盘等。从编程角度，还可以使用合适的数据结构和算法</p><p>代码层：采用高效算法，避免低效代码；应用层：多线程/多进程并发；数据库/缓存：分库分表并发；系统层：多节点、多套系统并发/负载均衡</p><h3 id=系统怎样做到高可用>系统怎样做到高可用</h3><p>高可用，指的是系统具有较高的无故障运行的能力。</p><h4 id=可用性的度量>可用性的度量</h4><p><strong>MTBF</strong>：平均故障间隔，代表两次故障的间隔时间，也就是系统正常运转的平均时间，这个时间越长，系统稳定性就越高。</p><p><strong>MTTR</strong>：故障的平均恢复时间，也可以理解为平均故障时间。这个值越小，故障对于用户的影响越小。</p><p>可用性与 MTBF 和 MTTR 的值息息相关，我们可以用下面的公式表示它们之间的关系：</p><blockquote><p>Availability = MTBF / (MTBF + MTTR)</p></blockquote><p>这个公式计算出的结果是一个比例，而这个比例代表着系统的可用性。一般来说，我们会使用几个九来描述系统的可用性。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://static001.geekbang.org/resource/image/73/75/73a87a9bc14a27c9ec9dfda1b72e1e75.jpg data-srcset="https://static001.geekbang.org/resource/image/73/75/73a87a9bc14a27c9ec9dfda1b72e1e75.jpg, https://static001.geekbang.org/resource/image/73/75/73a87a9bc14a27c9ec9dfda1b72e1e75.jpg 1.5x, https://static001.geekbang.org/resource/image/73/75/73a87a9bc14a27c9ec9dfda1b72e1e75.jpg 2x" data-sizes=auto alt=https://static001.geekbang.org/resource/image/73/75/73a87a9bc14a27c9ec9dfda1b72e1e75.jpg title=img></p><p>三个九之后，系统的年故障时间从 3 天锐减到 8 小时。到了四个九之后，年故障时间缩减到 1 小时之内。在这个级别的可用性下，你可能需要建立完善的运维值班体系、故障处理流程和业务变更流程。你可能还需要在系统设计上有更多的考虑。比如，在开发中你要考虑，如果发生故障，是否不用人工介入就能自动恢复。当然了，在工具建设方面，你也需要多加完善，以便快速排查故障原因，让系统快速恢复。</p><p>到达五个九之后，故障就不能靠人力恢复了。想象一下，从故障发生到你接收报警，再到你打开电脑登录服务器处理问题，时间可能早就过了十分钟了。所以这个级别的可用性考察的是系统的容灾和自动恢复的能力，让机器来处理故障，才会让可用性指标提升一个档次。</p><p>一般来说，我们的核心业务系统的可用性，需要达到四个九，非核心系统的可用性最多容忍到三个九。在实际工作中，你可能听到过类似的说法，只是不同级别，不同业务场景的系统对于可用性要求是不一样的。</p><h4 id=高可用系统的设计思路>高可用系统的设计思路</h4><h5 id=系统设计>系统设计</h5><p>“Design for failure”是我们做高可用系统设计时秉持的第一原则。在承担百万 QPS 的高并发系统中，集群中机器的数量成百上千台，单机的故障是常态，几乎每一天都有发生故障的可能。</p><p>未雨绸缪才能决胜千里。我们在做系统设计的时候，要把发生故障作为一个重要的考虑点，预先考虑如何自动化地发现故障，发生故障之后要如何解决。当然了，除了要有未雨绸缪的思维之外，我们还需要掌握一些具体的优化方法，比如 <strong>failover（故障转移）、超时控制以及降级和限流</strong>。</p><p><strong>failover-故障转移</strong></p><p>一般来说，发生failover的节点可能有两种情况：</p><ol><li>是在完全对等的节点之间做failover。</li><li>是在不完全对等的节点之间，即系统中存在主节点也存在备节点。</li></ol><p>在对等节点之间做 failover 相对来说简单些。在这类系统中所有节点都承担读写流量，并且节点中不保存状态，每个节点都可以作为另一个节点的镜像。在这种情况下，如果访问某一个节点失败，那么简单地随机访问另一个节点就好了。</p><p>比如Nginx upstream的failover机制。</p><p>针对不对等节点的 failover 机制会复杂很多。比方说我们有一个主节点，有多台备用节点，这些备用节点可以是热备（同样在线提供服务的备用节点），也可以是冷备（只作为备份使用），那么我们就需要在代码中控制如何检测主备机器是否故障，以及如何做主备切换。</p><p>使用最广泛的故障检测机制是“心跳”。你可以在客户端上定期地向主节点发送心跳包，也可以从备份节点上定期发送心跳包。当一段时间内未收到心跳包，就可以认为主节点已经发生故障，可以触发选主的操作。选主的结果需要在多个备份节点上达成一致，所以会使用某一种分布式一致性算法，比方说 Paxos，Raft。</p><p><strong>超时控制</strong></p><p>复杂的高并发系统通常会有很多的系统模块组成，同时也会依赖很多的组件和服务，比如说缓存组件，队列服务等等。它们之间的调用最怕的就是延迟而非失败，因为失败通常是瞬时的，可以通过重试的方式解决。而一旦调用某一个模块或者服务发生比较大的延迟，调用方就会阻塞在这次调用上，它已经占用的资源得不到释放。当存在大量这种阻塞请求时，调用方就会因为用尽资源而挂掉。</p><p>在系统开发的初期，超时控制通常不被重视，或者是没有方式来确定正确的超时时间。</p><p>既然要做超时控制，那么我们怎么来确定超时时间呢？</p><p>超时时间短了，会造成大量的超时错误，对用户体验产生影响；超时时间长了，又起不到作用。我建议你通过收集系统之间的调用日志，统计比如说 99% 的响应时间是怎样的，然后依据这个时间来指定超时时间。如果没有调用的日志，那么你只能按照经验值来指定超时时间。不过，无论你使用哪种方式，超时时间都不是一成不变的，需要在后面的系统维护过程中不断地修改。</p><p>超时控制实际上就是不让请求一直保持，而是在经过一定时间之后让请求失败，释放资源给接下来的请求使用。这对于用户来说是有损的，但是却是必要的，因为它牺牲了少量的请求却保证了整体系统的可用性。而我们还有另外两种有损的方案能保证系统的高可用，它们就是降级和限流。</p><p><strong>降级</strong></p><p>降级是为了保证核心服务的稳定而牺牲非核心服务的做法。</p><p><strong>限流</strong></p><p>它通过对并发的请求进行限速来保护系统。</p><p>比如对于 Web 应用，我限制单机只能处理每秒 1000 次的请求，超过的部分直接返回错误给客户端。虽然这种做法损害了用户的使用体验，但是它是在极端并发下的无奈之举，是短暂的行为，因此是可以接受的。比方说，你上了一个新的功能，由于设计方案的问题，数据库的慢请求数翻了一倍，导致系统请求被拖慢而产生故障。</p><h5 id=系统运维>系统运维</h5><p>在系统运维层面，我们可以从<strong>灰度发布</strong>、<strong>故障演练</strong>两个方面来考虑如何提升系统的可用性。</p><p>在业务平稳运行过程中，系统是很少发生故障的，90% 的故障是发生在上线变更阶段的。比方说，你上了一个新的功能，由于设计方案的问题，数据库的慢请求数翻了一倍，导致系统请求被拖慢而产生故障。</p><p>为了提升系统的可用性，重视变更管理尤为重要。而除了提供必要回滚方案，以便在出现问题时快速回滚恢复之外，另一个主要的手段就是灰度发布。</p><p><strong>灰度发布</strong></p><p>灰度发布指的是系统的变更不是一次性地推到线上的，而是按照一定比例逐步推进的。一般情况下，灰度发布是以机器维度进行的。比方说，我们先在 10% 的机器上进行变更，同时观察 Dashboard 上的系统性能指标以及错误日志。如果运行了一段时间之后系统指标比较平稳并且没有出现大量的错误日志，那么再推动全量变更。</p><p>灰度发布是在系统正常运行条件下，保证系统高可用的运维手段，那么我们如何知道发生故障时系统的表现呢？这里就要依靠另外一个手段：故障演练。</p><p><strong>故障演练</strong></p><p>故障演练指的是对系统进行一些破坏性的手段，观察在出现局部故障时，整体的系统表现是怎样的，从而发现系统中存在的，潜在的可用性问题。</p><p>一个复杂的高并发系统依赖了太多的组件，比方说磁盘，数据库，网卡等，这些组件随时随地都可能会发生故障，而一旦它们发生故障，会不会如蝴蝶效应一般造成整体服务不可用呢？我们并不知道，因此，故障演练尤为重要。</p><h4 id=小结>小结</h4><p>从开发和运维角度上来看，提升可用性的方法是不同的：</p><ul><li>开发注重的是如何处理故障，关键词是冗余和取舍。冗余指的是有备用节点，集群来顶替出故障的服务，比如文中提到的故障转移，还有多活架构等等；取舍指的是丢卒保车，保障主体服务的安全。</li><li>运维角度注重的是如何避免故障的发生，比如更关注变更管理以及如何做故障的演练。</li></ul><p>两者结合起来才能组成一套完善的高可用体系。</p><p>你还需要注意的是，提高系统的可用性有时候是以牺牲用户体验或者是牺牲系统性能为前提的，也需要大量人力来建设相应的系统，完善机制。所以我们要把握一个度，不该做过度的优化。就像我在文中提到的，核心系统四个九的可用性已经可以满足需求，就没有必要一味地追求五个九甚至六个九的可用性。</p><p>另外，一般的系统或者组件都是追求极致的性能的，那么有没有不追求性能，只追求极致的可用性的呢？答案是有的。比如配置下发的系统，它只需要在其它系统启动时提供一份配置即可，所以秒级返回也可，十秒钟也 OK，无非就是增加了其它系统的启动时间而已。但是，它对可用性的要求是极高的，甚至会到六个九，原因是配置可以获取的慢，但是不能获取不到。我给你举这个例子是想让你了解，可用性和性能有时候是需要做取舍的，但如何取舍就要视不同的系统而定，不能一概而论了。</p><p>感谢老师的分享，对于文章中的心跳检测我个人认为其实是需要做区分的，心跳检测是区分内部检测和外部检测，外部检测伴随着随机性，有时候可能系统的响应时间或者IO已经出现拐点了，但是心跳机制还是有可能会收到响应包，从而被错误的当做系统是“正常”的，所以很多高可用的系统基本都是采用内部心跳机制，比如kafka中的通过zookeeper的watch机制来做检测来做broker中controller的failover，mysql里面是通过内部的performance_schema库中的file_summary_by_event_name表来做监控，都是为了避免外部监测的随机性所带来的影响，不过对于老师提出的可用性和高性能当中出取舍无比赞同，不同的解决方案总是伴随着优势和劣势，无非是优势所带来的效果比劣势对业务更有帮助而已，比如之前在工作中碰到过mysql主从同步有延迟的情况，mysql的高可能性其实就是靠主备同步的数据一致性来保证的，经过排查发现，业务代码中存在有操作大事务的SQL语句，这个导致从库拿到binlog去做重放的时候，时间周期拖长了，从而影响了业务，另外一个情况就是，在做秒杀的时候，除了采用redis外，还会对用户划分成两部分，一部分用户是来了以后直接报秒杀已结束，另外一部分用户才会进入秒杀的逻辑中，心里觉得还是挺对不住那部分抢不到秒杀的用户的，所以，有时候我们需要在可用性和可靠性上需要做一个取舍，没有十全十美的技术解决方案。除此之外，其实大部分的时候采用的策略是需要对业务尽量是无损的，不然运营和产品会请你喝茶的，最后想跟老师提个建议，就是希望后面的一些篇章，能否尽量将之前提到过的知识点实际运用起来或者说串联起来，并且提供一些实际场景的解决方案细节，我举个例子，比如电商秒杀，秒杀如果只采用mysql去做库存判断，会出现什么的问题，为了避免这样的问题出现，该采用什么系统，假如如果采用redis，为什么？redis的一些操作是原子操作，原子操作是什么？如何保证？类似这样的方案的优缺点和延伸，这样的话既能有效的总结之前学习到的知识点，能够将整个知识网络做一个规整，还能了解到每个技术方案的不足加深理解，从而明白什么样的场景说不能采用当前的技术方案的，还有就是在采用技术方案的同时，说明下采用方案的一些基础的理论，知识，比如cas，或者分布式系统中的cap理论，这样对于基础知识欠缺的人，在学习的时候也可以去补一些平时没有刻意学习的知识，上面是我举的例子，再次谢谢老师的分享，期待后面老师的干货文章。</p><h3 id=如何让系统易于扩展>如何让系统易于扩展</h3><p>从架构设计上来说，高可扩展性是一个设计的指标，它表示可以通过增加机器的方式来线性提高系统的处理能力，从而承担更高的流量和并发。</p><p>为应对突发的流量？架构的改造已经来不及了，最快的方式就是堆机器，但我们要保证，扩容了三倍的机器之后，相应的我们的系统也能支撑三倍的流量。但这不是一个显而易见的事情。</p><h4 id=为什么提升扩展性会很复杂>为什么提升扩展性会很复杂</h4><p>在单机系统，通过增加处理核心的方式来增加系统并行处理能力并不总是生效的，因为当并行任务较多时，系统会因为争抢资源而达到性能上的拐点，系统处理能力不升反降。</p><p>而对于由多台机器组成的集群系统来说也是如此。集群系统中，不同的系统分层上可能存在一些“瓶颈点”，这些瓶颈点制约着系统的横向扩展能力。那么，我们的</p><p>系统中存在哪些服务会成为制约系统扩展的重要因素呢？</p><p>其实，无状态的服务和组件更易于扩展，而像 MySQL 这种存储服务是有状态的，就比较难以扩展。因为向存储集群中增加或者减少机器时，会涉及大量数据的迁移，而一般传统的关系型数据库都不支持。这就是为什么提升系统扩展性会很复杂的主要原因。</p><p>除此之外，我们需要站在整体架构的角度，而不仅仅是业务服务器的角度来考虑系统的扩展性 。所以说，<strong>数据库、缓存、依赖的第三方、负载均衡、交换机带宽等等</strong>都是系统扩展时需要考虑的因素。我们要知道系统并发到了某一个量级之后，哪一个因素会成为我们的瓶颈点，从而针对性地进行扩展。</p><h4 id=高可扩展性的设计思路>高可扩展性的设计思路</h4><p>拆分是提升系统扩展性最重要的一个思路，它会把庞杂的系统拆分成独立的，有单一职责的模块。相对于大系统来说，考虑一个一个小模块的扩展性当然会简单一些。将复杂的问题简单化，这就是我们的思路。</p><p>一般一开始，我们的部署方式都是简单的三层部署架构：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://static001.geekbang.org/resource/image/58/a6/5803451931917e0806c37c39802410a6.jpg data-srcset="https://static001.geekbang.org/resource/image/58/a6/5803451931917e0806c37c39802410a6.jpg, https://static001.geekbang.org/resource/image/58/a6/5803451931917e0806c37c39802410a6.jpg 1.5x, https://static001.geekbang.org/resource/image/58/a6/5803451931917e0806c37c39802410a6.jpg 2x" data-sizes=auto alt=https://static001.geekbang.org/resource/image/58/a6/5803451931917e0806c37c39802410a6.jpg title=img></p><p>负载均衡负载请求的转发，应用服务器负载业务的处理，数据库负责数据的存储</p><p>但对于不同类型的模块，我们在拆分上遵循的原则是不一样的。</p><p><strong>存储层的扩展性</strong></p><p>存储拆分首先考虑的维度是业务维度。</p><p>按照业务拆分，在一定程度上提升了系统的扩展性，但系统运行时间长了之后，单一的业务数据库在容量和并发请求量上仍然会超过单机的限制。这时，我们就需要针对数据库做第二次拆分。</p><p>普通的分库分表</p><p>这次拆分是按照数据特征做水平的拆分，比如说我们可以给用户库增加两个节点，然后按照某些算法将用户的数据拆分到这三个库里面。</p><p>水平拆分之后，我们就可以让数据库突破单机的限制了。但这里要注意，我们不能随意地增加节点，因为一旦增加节点就需要手动地迁移数据，成本还是很高的。所以基于长远的考虑，我们最好一次性增加足够的节点以避免频繁的扩容。</p><p>当数据库按照业务和数据维度拆分之后，我们尽量不要使用事务。因为当一个事务中同时更新不同的数据库时，需要使用二阶段提交，来协调所有数据库要么全部更新成功，要么全部更新失败。这个协调的成本会随着资源的扩展不断升高，最终达到无法承受的程度。</p><p><strong>业务层的扩展性</strong></p><p>我们一般会从三个维度考虑业务层的拆分方案，它们分别是：业务维度，重要性维度和请求来源维度。</p><p>首先，我们需要把相同业务的服务拆分成单独的业务池。</p><p>每个业务依赖独自的数据库资源，不会依赖其它业务的数据库资源。这样当某一个业务的接口成为瓶颈时，我们只需要扩展业务的池子，以及确认上下游的依赖方就可以了，这样就大大减少了扩容的复杂度。</p><p>除此之外，我们还可以根据业务接口的重要程度，把业务分为核心池和非核心池。</p><p>最后，你还可以根据接入客户端类型的不同做业务池的拆分。比如说，服务于客户端接口的业务可以定义为外网池，服务于小程序或者 HTML5 页面的业务可以定义为 H5 池，服务于内部其它部门的业务可以定义为内网池，等等。</p><p>未做拆分的系统虽然可扩展性不强，但是却足够简单，无论是系统开发还是运行维护都不需要投入很大的精力。拆分之后，需求开发需要横跨多个系统多个小团队，排查问题也需要涉及多个系统，运行维护上，可能每个子系统都需要有专人来负责，对于团队是一个比较大的考验。这个考验是我们必须要经历的一个大坎，需要我们做好准备。</p><h2 id=数据库篇>数据库篇</h2><h3 id=数据库连接池>数据库连接池</h3><p>当我们进行一个新项目的时候，肯定是最简单的架构：前后端分离，前端一台服务器部署前端项目，后端一台服务器部署web应用，最后数据库一台服务器。</p><p>只是随着业务的复杂提高，架构做了叠加，看起来越来越复杂，但本质上还是简单的三层架构。</p><p>但是，当系统上线一段时间后，用户越来越多，流量越来越大，系统访问就变得越来越慢了。</p><p>分析程序日志之后，你发现，系统变慢的原因出现在和数据库的交互上。我们访问数据库获取数据都是先建立连接，请求数据，关闭连接。这种调用方式下，每次执行sql都需要重新建立连接，所以，你怀疑，是不是频繁地建立数据库连接耗费时间长导致了访问慢的问题。</p><p>那么，我们怎么优化数据库连接呢，通过查找，发现，可以使用数据库连接池的方案来实现。</p><p>数据库连接池有两个重要的参数：最小连接数和最大连接数，它们控制着从连接池获取连接的流程：</p><ul><li>如果当前连接数小于最小连接数，就新建连接</li><li>如果当前有空闲连接，就使用空闲连接</li><li>如果当前没有空闲连接，且当前连接数小于最大连接数，就新建连接</li><li>如果当前连接大于等于最大连接数，就按照配置中设定的时间等待旧的连接可用</li><li>如果等待超过了这个设定时间则向用户抛出错误</li></ul><p>对于数据库连接池，根据我的经验，一般在线上我建议最小连接数控制在 10 左右，最大连接数控制在 20～30 左右即可。</p><p>数据库连接池的问题：</p><ol><li>数据库的ip发生了变更，但当前使用的连接还是旧ip，导致使用这个连接查询就会报错</li><li>MySQL有个参数 wait_timeout，控制着当前数据库连接闲置多长时间后，数据库就会主动地关闭这条连接。所以当我们使用这条连接时就会发生错误。</li></ol><p>那么，怎么保证数据库连接池的连接可用呢？</p><p>最简单的方式就是发送心跳检测，启动一个线程来定期检测连接池中的连接是否可用，比如使用连接发送“select 1”的命令给数据库看是否会抛出异常，如果抛出异常则将这个连接从连接池中移除，并且尝试关闭。目前 C3P0 连接池可以采用这种方式来检测连接是否可用，也是比较推荐的方式。</p><p>还有一种是在获取到连接之后，先校验连接是否可用，如果可用才会执行 SQL 语句。比如 DBCP 连接池的 testOnBorrow 配置项，就是控制是否开启这个验证。这种方式在获取连接时会引入多余的开销，在线上系统中还是尽量不要开启，在测试服务上可以使用。</p><p><strong>用线程池预先创建线程</strong></p><p>JDK提供了线程池技术来让我们可以创建线程池。JDK 1.5 中引入的 ThreadPoolExecutor 就是一种线程池的实现，它有两个重要的参数：coreThreadCount 和 maxThreadCount，这两个参数控制着线程池的执行过程。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://static001.geekbang.org/resource/image/d4/99/d4f7b06f3c28d88d17b5e2d4b49b6999.jpg data-srcset="https://static001.geekbang.org/resource/image/d4/99/d4f7b06f3c28d88d17b5e2d4b49b6999.jpg, https://static001.geekbang.org/resource/image/d4/99/d4f7b06f3c28d88d17b5e2d4b49b6999.jpg 1.5x, https://static001.geekbang.org/resource/image/d4/99/d4f7b06f3c28d88d17b5e2d4b49b6999.jpg 2x" data-sizes=auto alt=https://static001.geekbang.org/resource/image/d4/99/d4f7b06f3c28d88d17b5e2d4b49b6999.jpg title=img></p><p>使用线程池需要注意：</p><p>JDK 实现的这个线程池优先把任务放入队列暂存起来，而不是创建更多的线程，它比较适用于执行 CPU 密集型的任务，也就是需要执行大量 CPU 运算的任务。这是为什么呢？因为执行 CPU 密集型的任务时 CPU 比较繁忙，因此只需要创建和 CPU 核数相当的线程就好了，多了反而会造成线程上下文切换，降低任务执行效率。所以当前线程数超过核心线程数时，线程池不会增加线程，而是放在队列里等待核心线程空闲下来。</p><p>但是，我们平时开发的 Web 系统通常都有大量的 IO 操作，比方说查询数据库、查询缓存等等。任务在执行 IO 操作的时候 CPU 就空闲了下来，这时如果增加执行任务的线程数而不是把任务暂存在队列中，就可以在单位时间内执行更多的任务，大大提高了任务执行的吞吐量。所以你看 Tomcat 使用的线程池就不是 JDK 原生的线程池，而是做了一些改造，当线程数超过 coreThreadCount 之后会优先创建线程，直到线程数到达 maxThreadCount，这样就比较适合于 Web 系统大量 IO 操作的场景了，你在实际使用过程中也可以参考借鉴。</p><p>其次，线程池中使用的队列的堆积量也是我们需要监控的重要指标，对于实时性要求比较高的任务来说，这个指标尤为关键。</p><p>我在实际项目中就曾经遇到过任务被丢给线程池之后，长时间都没有被执行的诡异问题。</p><p>最后，如果你使用线程池请一定记住不要使用无界队列（即没有设置固定大小的队列）。也许你会觉得使用了无界队列后，任务就永远不会被丢弃，只要任务对实时性要求不高，反正早晚有消费完的一天。但是，大量的任务堆积会占用大量的内存空间，一旦内存空间被占满就会频繁地触发 Full GC，造成服务不可用，我之前排查过的一次 GC 引起的宕机，起因就是系统中的一个线程池使用了无界队列。</p><p><strong>重点</strong></p><ul><li>池子的最大值和最小值的设置很重要，初期可以依据经验来设置，后面还是需要根据实际运行情况做调整。</li><li>池子中的对象需要在使用之前预先初始化完成，这叫做池子的预热，比方说使用线程池时就需要预先初始化所有的核心线程。如果池子未经过预热可能会导致系统重启后产生比较多的慢请求。</li><li>池化技术核心是一种空间换时间优化方法的实践，所以要关注空间占用情况，避免出现空间过度使用出现内存泄露或者频繁垃圾回收等问题。</li></ul><h3 id=数据库优化方案一主从分离>数据库优化方案一：主从分离</h3><p>经过数据库连接池的优化之后，你的性能提升了80%，但是，此时你的数据库还是单机部署，在 4 核 8G 的机器上运行 MySQL 5.7 时，大概可以支撑 500 的 TPS 和 10000 的 QPS。随着查询请求的进一步增大，我们需要依靠主从分离的技术，来增加查询请求的并发量。</p><h4 id=主从读写分离>主从读写分离</h4><p>其实，大部分的访问模型是读多写少，读写请求量差距可能达到几个数量级。因此，我们优先考虑数据库如何抵抗更高的查询请求。那么首先就要把读写流量分离开，这样才能更方便的针对读流量进行扩展，这就是我们所说的主从读写分离。</p><p><strong>主从读写分离的两个关键点</strong></p><ol><li>数据的拷贝，主从复制</li><li>数据的访问，如何屏蔽主从分离带来的访问数据库方式的变化</li></ol><h5 id=主从复制>主从复制</h5><p>MySQL 的主从复制是依赖于 binlog 的，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上二进制日志文件。主从复制就是将 binlog 中的数据从主库传输到从库上，一般这个过程是异步的，即主库上的操作不会等待 binlog 同步的完成。</p><p>主从复制的过程是这样的：首先从库在连接到主节点时会创建一个 IO 线程，用以请求主库更新的 binlog，并且把接收到的 binlog 信息写入一个叫做 relay log 的日志文件中，而主库也会创建一个 log dump 线程来发送 binlog 给从库；同时，从库还会创建一个 SQL 线程读取 relay log 中的内容，并且在从库中做回放，最终实现主从的一致性。这是一种比较常见的主从复制方式。</p><p>在这个方案中，使用独立的 log dump 线程是一种异步的方式，可以避免对主库的主体更新流程产生影响，而从库在接收到信息后并不是写入从库的存储中，是写入一个 relay log，是避免写入从库实际存储会比较耗时，最终造成从库和主库延迟变长。</p><p>做了主从复制之后，我们就可以在写入时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响到读请求的执行。同时呢，在读流量比较大的情况下，我们可以部署多个从库共同承担读流量，这就是所说的“一主多从”部署方式，在你的垂直电商项目中就可以通过这种方式来抵御较高的并发读流量。另外，从库也可以当成一个备库来使用，以避免主库故障导致数据丢失。</p><p>那么你可能会说，是不是我无限制地增加从库的数量就可以抵抗大量的并发呢？实际上并不是的。因为随着从库数量增加，从库连接上来的 IO 线程比较多，主库也需要创建同样多的 log dump 线程来处理复制的请求，对于主库资源消耗比较高，同时受限于主库的网络带宽，所以在实际使用中，一般一个主库最多挂 3～5 个从库。</p><p>主从复制的缺陷</p><p>主从同步的延迟，这种延迟有时候会对业务产生一定的影响。</p><p>解决方案：</p><p>数据的冗余</p><p>使用缓存</p><p>查询主库</p><p>另外，主从同步的延迟，是我们排查问题时很容易忽略的一个问题。所以，一般我们会把从库落后的时间作为一个重点的数据库指标做监控和报警，正常的时间是在毫秒级别，一旦落后的时间达到了秒级别就需要告警了。</p><h5 id=访问数据>访问数据</h5><p>我们已经使用主从复制的技术将数据复制到了多个节点，也实现了数据库读写的分离，这时，对于数据库的使用方式发生了变化。以前只需要使用一个数据库地址就好了，现在需要使用一个主库地址和多个从库地址，并且需要区分写入操作和查询操作，如果结合下一节课中要讲解的内容“分库分表”，复杂度会提升更多。为了降低实现的复杂度，业界涌现了很多数据库中间件来解决数据库的访问问题，这些中间件可以分为两类。</p><p><strong>第一类：通过代码形式内嵌运行在应用程序内部，多数据源管理。</strong></p><p>这一类中间件的优点是简单易用，没有多余的部署成本，因为它是植入到应用程序内部，与应用程序一同运行的，所以比较适合运维能力较弱的小团队使用；缺点是缺乏多语言的支持，目前业界这一类的主流方案除了 TDDL，还有早期的网易 DDB，它们都是 Java 语言开发的，无法支持其他的语言。另外，版本升级也依赖使用方更新，比较困难。</p><p><strong>第二类：单独部署的代理层方案。</strong></p><p>这一类中间件部署在独立的服务器上，业务代码如同在使用单一数据库一样使用它，实际上它内部管理着很多的数据源，当有数据库请求时，它会对 SQL 语句做必要的改写，然后发往指定的数据源。</p><p>它一般使用标准的 MySQL 通信协议，所以可以很好地支持多语言。由于它是独立部署的，所以也比较方便进行维护升级，比较适合有一定运维能力的大中型团队使用。它的缺陷是所有的 SQL 语句都需要跨两次网络：从应用到代理层和从代理层到数据源，所以在性能上会有一些损耗。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 0001-01-01</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/ class=next rel=next title><i class="fas fa-angle-right fa-fw"></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.81.0">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i>LoveIt</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank></a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i></a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><script type=text/javascript src=/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:10},comment:{}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>